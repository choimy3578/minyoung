import tensorflow as tf
import numpy as np
 
x1 = [ 73.,  93.,  89.,  96.,  73.]
x2 = [ 80.,  88.,  91.,  98.,  66.]
x3 = [ 75.,  93.,  90., 100.,  70.]
Y  = [152., 185., 180., 196., 142.]

# random weights
w1 = tf.Variable(tf.random_normal([1]))    변수의 개수(3개)만큼 w도 3개가 필요하다.
w2 = tf.Variable(tf.random_normal([1]))    초기값을 1로 둔다.
w3 = tf.Variable(tf.random_normal([1]))
b  = tf.Variable(tf.random_normal([1]))

learning_rate = 0.000001        learning_rate를 작은 값으로 준다.

for i in range(1000+1):         1001번 반복한다.
    # tf.GradientTape() to record the gradient of the cost function
    with tf.GradientTape() as tape:          tape에 모든 정보를 저장한다.
        hypothesis = w1 * x1 +  w2 * x2 + w3 * x3 + b       * hypothesis 함수
        cost = tf.reduce_mean(tf.square(hypothesis - Y))   
  hypothesis에서 Y를 뺀 값(오차)에 제곱을 하여 cost 값으로 정한다.
    # calculates the gradients of the cost 
    w1_grad, w2_grad, w3_grad, b_grad = tape.gradient(cost, [w1, w2, w3, b])
  tape에 gradient를 호출해서 cost 함수에 대한 네개의 변수들의 기울기 값을 구한다.
    # update w1,w2,w3 and b
    w1.assign_sub(learning_rate * w1_grad)           각각의 gradient 값을 w1, w2, w3에 업데이트
    w2.assign_sub(learning_rate * w2_grad)           learning rate 값과 w_grade 값을 곱해서 그 값을 빼고 할당해준다.
    w3.assign_sub(learning_rate * w3_grad)
    b.assign_sub(learning_rate * b_grad)

    if i % 50 == 0:                 50번마다 cost 값을 출력한다.
      print("{:5} | {:12.4f}".format(i, cost.numpy()))
      
 # 결과값
    0 |   11325.9121
   50 |     135.3618
  100 |      11.1817
  150 |       9.7940
  200 |       9.7687
  250 |       9.7587
  300 |       9.7489
  350 |       9.7389            급격히 줄어들다가 일정 순간부터 cost 값이 일정하다.
  400 |       9.7292
  450 |       9.7194
  500 |       9.7096
  550 |       9.6999
  600 |       9.6903
  650 |       9.6806
  700 |       9.6709
  750 |       9.6612
  800 |       9.6517
  850 |       9.6421
  900 |       9.6325
  950 |       9.6229
 1000 |       9.6134
 
 <Matrix 이용하기>
 
 data = np.array([
    # X1,   X2,    X3,   y
    [ 73.,  80.,  75., 152. ],
    [ 93.,  88.,  93., 185. ],
    [ 89.,  91.,  90., 180. ],
    [ 96.,  98., 100., 196. ],
    [ 73.,  66.,  70., 142. ]
], dtype=np.float32)

# slice data
X = data[:, :-1]            x data는 slicing을 통해서 5행 3열짜리 데이터만큼 가진다.
y = data[:, [-1]]           y는 slicing을 통해서 5행 1열짜리 행렬이 된다.

W = tf.Variable(tf.random_normal([3, 1]))          w는 x를 받아서 3(입력) by 1(출력) 행렬이 된다.
b = tf.Variable(tf.random_normal([1]))       

learning_rate = 0.000001

# hypothesis, prediction function
def predict(X):
    return tf.matmul(X, W) + b  
predict 함수는 x, w로 정의된다. b를 더해준다.


n_epochs = 2000           20001회 epoch를 반복한다.
for i in range(n_epochs+1):
    # tf.GradientTape() to record the gradient of the cost function
    with tf.GradientTape() as tape:                  tape에 모든 정보를 저장한다.
        cost = tf.reduce_mean((tf.square(predict(X) - y)))
 예측한 x에 y를 뺀 값을 제곱하고 cost 값으로 정한다.

    # calculates the gradients of the loss
    W_grad, b_grad = tape.gradient(cost, [W, b])
tape에 gradient를 호출해서 cost 함수에 대한 기울기 값을 구한다.
    # updates parameters (W and b)
    W.assign_sub(learning_rate * W_grad)             learning_rate값과 grade 값을 곱한 값을 빼서 할당해준다.
    b.assign_sub(learning_rate * b_grad)
    
    if i % 100 == 0:                100번에 한 번씩 cost 값을 출력한다.
        print("{:5} | {:10.4f}".format(i, cost.numpy()))
        
# 결과값

epoch | cost
    0 |  5455.5903      
  100 |    31.7443
  200 |    30.9326
  300 |    30.7894
  400 |    30.6468
  500 |    30.5055
  600 |    30.3644
  700 |    30.2242
  800 |    30.0849
  900 |    29.9463
 1000 |    29.8081
 1100 |    29.6710
 1200 |    29.5348
 1300 |    29.3989
 1400 |    29.2641
 1500 |    29.1299
 1600 |    28.9961
 1700 |    28.8634
 1800 |    28.7313
 1900 |    28.5997
 2000 |    28.4689
