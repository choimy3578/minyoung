<tensorflow로 구현>

import tensorflow as tf      tensorflow를 실행한다.
import numpy as np           numpy를 import한다.
X = np.array([1, 2, 3])      x와 y 데이터를 동일하게 1,2,3으로 준비한다.
Y = np.array([1, 2, 3])

def cost_func(W, X, Y):      데이터 x, y에 대해서 w값이 주어졌을 때 cost함수를 계산
    hypothesis = X * W       
    return tf.reduce_mean(tf.square(hypothesis - Y)) 
 hypothesis에 Y를 빼고 제곱하고 평균을 낸 것 => cost function이다.

W_values = np.linspace(-3, 5, num=15)         -3에서 5까지 15개의 구간으로 나눈다.
cost_values = []                              그 값을 리스트로 받는다.

for feed_W in W_values:     받은 리스트 값을 하나씩 꼽는다.
    curr_cost = cost_func(feed_W, X, Y)       받은 값을 weight 값으로 사용하여 cost가 weight에 따라서 어떻게 변하는지 기록
    cost_values.append(curr_cost)         
    print("{:6.3f} | {:10.5f}".format(feed_W, curr_cost))        기록한 값을 출력한다.

# 결과값
-3.000 |   74.66667
-2.429 |   54.85714
-1.857 |   38.09524
-1.286 |   24.38095
-0.714 |   13.71429
-0.143 |    6.09524
 0.429 |    1.52381
 1.000 |    0.00000
 1.571 |    1.52381
 2.143 |    6.09524
 2.714 |   13.71429
 3.286 |   24.38095
 3.857 |   38.09524
 4.429 |   54.85714
 5.000 |   74.66667
 
<Gradient descent 구현>
 
tf.set_random_seed(0)        random_seed를 특정한 값으로 초기화시킨다.
x_data = [1., 2., 3., 4.]    
y_data = [1., 3., 5., 7.]

W = tf.Variable(tf.random_normal([1], -100., 100.))     
random_normal에서 정규분포를 따르는 random number, 한 개짜리로 변수를 만들어서 w에 할당하여 정의한다.

for step in range(300):       
    hypothesis = W * X
    cost = tf.reduce_mean(tf.square(hypothesis - Y))
 hypothesis에 Y를 빼고 제곱하고 평균을 낸 것 => cost function이다.


    alpha = 0.01          gradient descent를 300회 반복한다.
    gradient = tf.reduce_mean(tf.multiply(tf.multiply(W, X) - Y, X))
(W, X) - Y를 한 것에 X를 곱한 것에 평균을 구한 것이 gradient이다.
    descent = W - tf.multiply(alpha, gradient)            gradient값에 alpha값을 곱하고 W값을 빼준다.
    W.assign(descent)      descent가 새로운 W값으로 할당해준다.
    
    if step % 10 == 0:       10번에 한 번씩 cost값과 W값을 출력한다.
        print('{:5} | {:10.4f} | {:10.6f}'.format(
            step, cost.numpy(), W.numpy()[0]))
   
  #결과값
    0 | 11716.3086 |  48.767971
   10 |  4504.9126 |  30.619968
   20 |  1732.1364 |  19.366755
   30 |   666.0052 |  12.388859
   40 |   256.0785 |   8.062004
   50 |    98.4620 |   5.379007
   60 |    37.8586 |   3.715335
   70 |    14.5566 |   2.683725
   80 |     5.5970 |   2.044044
   90 |     2.1520 |   1.647391
  100 |     0.8275 |   1.401434
  110 |     0.3182 |   1.248922
  120 |     0.1223 |   1.154351
  130 |     0.0470 |   1.095710
  140 |     0.0181 |   1.059348
  150 |     0.0070 |   1.036801
  160 |     0.0027 |   1.022819
  170 |     0.0010 |   1.014150
  180 |     0.0004 |   1.008774
  190 |     0.0002 |   1.005441
  200 |     0.0001 |   1.003374
  210 |     0.0000 |   1.002092
  220 |     0.0000 |   1.001297
  230 |     0.0000 |   1.000804
  240 |     0.0000 |   1.000499
  250 |     0.0000 |   1.000309
  260 |     0.0000 |   1.000192
  270 |     0.0000 |   1.000119
  280 |     0.0000 |   1.000074
  290 |     0.0000 |   1.000046
